# CTI Scraper

A modern threat intelligence collection and analysis platform that automatically gathers, processes, and analyzes security content from RSS feeds and web sources. Built with FastAPI, PostgreSQL, and Celery for scalable threat intelligence operations.

## ğŸš€ Features

- **Multi-Source Collection**: RSS feeds + intelligent web scraping with structured data extraction
- **Content Processing**: Advanced cleaning, normalization, deduplication, and quality scoring
- **Modern Web Interface**: FastAPI-powered dashboard with search, filtering, and analytics
- **Scalable Architecture**: PostgreSQL storage with async operations and Celery background tasks
- **Robots.txt Compliance**: Respectful crawling with configurable rate limiting per source
- **Source Tiering**: Priority-based collection system (premium/standard/basic tiers)
- **Threat Intelligence Focus**: Specialized for cybersecurity content analysis

## ğŸ“‹ Quick Start

### Prerequisites
- Docker Desktop
- Git

### Development Setup
```bash
# Clone the repository
git clone https://github.com/dfirtnt/CTIScraper.git
cd CTIScraper

# Start development environment
./start_development.sh

# Initialize sources
./run_cli.sh init

# Start collecting content
./run_cli.sh collect --dry-run
```

### Production Deployment
```bash
# Start production stack
./start_production.sh
```

**Access Points:**
- Web UI: http://localhost:8000
- API: http://localhost:8000/api/*
- Health Check: http://localhost:8000/health

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web Interface â”‚    â”‚  Background     â”‚    â”‚   Data Storage  â”‚
â”‚   (FastAPI)     â”‚â—„â”€â”€â–ºâ”‚  Tasks (Celery) â”‚â—„â”€â”€â–ºâ”‚  (PostgreSQL)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Content       â”‚    â”‚   Source        â”‚    â”‚   Redis Cache  â”‚
â”‚   Collection    â”‚    â”‚   Management    â”‚    â”‚   & Queue       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
src/
â”œâ”€â”€ web/                 # FastAPI application
â”‚   â”œâ”€â”€ modern_main.py   # Main web application
â”‚   â””â”€â”€ templates/       # HTML templates
â”œâ”€â”€ core/                # Content ingestion engine
â”‚   â”œâ”€â”€ rss_parser.py    # RSS/Atom feed processing
â”‚   â”œâ”€â”€ modern_scraper.py # Structured data extraction
â”‚   â”œâ”€â”€ fetcher.py       # Multi-strategy content fetching
â”‚   â””â”€â”€ processor.py     # Content processing pipeline
â”œâ”€â”€ database/            # Data layer
â”‚   â”œâ”€â”€ models.py        # SQLAlchemy models
â”‚   â”œâ”€â”€ async_manager.py # Async database operations
â”‚   â””â”€â”€ manager.py       # Sync database operations
â”œâ”€â”€ worker/              # Background task processing
â”œâ”€â”€ utils/               # Shared utilities
â””â”€â”€ cli/                 # Command-line interface

config/
â”œâ”€â”€ sources.yaml         # Source definitions and configuration
â”œâ”€â”€ models.yaml          # Model configuration
â””â”€â”€ recommended_models.yaml # Recommended settings
```

## ğŸ”§ Configuration

### Source Configuration (`config/sources.yaml`)
```yaml
sources:
  - id: "thehackernews"
    name: "The Hacker News"
    url: "https://thehackernews.com/"
    rss_url: "https://feeds.feedburner.com/TheHackersNews"
    tier: 2  # Source priority (1=premium, 2=standard, 3=basic)
    check_frequency: 3600
    active: true
    robots:
      enabled: true
      user_agent: "CTIScraper/2.0"
      respect_delay: true
      max_requests_per_minute: 10
```

### Environment Variables
```bash
# Database
DATABASE_URL=postgresql+asyncpg://cti_user:password@postgres:5432/cti_scraper

# Redis
REDIS_URL=redis://:password@redis:6379/0

# OpenAI (optional)
CHATGPT_API_KEY=your_openai_api_key_here
CHATGPT_API_URL=https://api.openai.com/v1/chat/completions
```

## ğŸ› ï¸ CLI Commands

```bash
# Initialize sources from configuration
./run_cli.sh init --config config/sources.yaml

# Collect content from all sources
./run_cli.sh collect --dry-run

# Monitor sources continuously
./run_cli.sh monitor --interval 300

# List active sources
./run_cli.sh sources list --active

# Export articles
./run_cli.sh export --format json --days 7

# Show system statistics
./run_cli.sh stats
```

## ğŸ” API Endpoints

### Web Interface
- `GET /` - Dashboard with statistics and recent articles
- `GET /articles` - Article listing with search and filters
- `GET /articles/{id}` - Detailed article view
- `GET /sources` - Source management interface

### JSON API
- `GET /health` - Service health status
- `GET /api/articles` - List articles with pagination
- `GET /api/articles/{id}` - Article details
- `GET /api/sources` - Source information
- `POST /api/sources/{id}/toggle` - Toggle source status

## ğŸ§ª Testing

```bash
# Run all tests
pytest -q

# Run specific test categories
pytest tests/unit/
pytest tests/integration/
pytest tests/api/
```

## ğŸ“š Documentation

Comprehensive documentation is available in the `docs/` directory:

- **[Documentation Overview](docs/README.md)** - Complete documentation index
- **[Docker Architecture](docs/deployment/DOCKER_ARCHITECTURE.md)** - Docker setup guide
- **[Testing Guide](docs/development/TESTING_GUIDE.md)** - Testing documentation
- **[Database Queries](docs/development/DATABASE_QUERY_GUIDE.md)** - Database operations

## ğŸ”’ Security Features

- **Robots.txt Compliance**: Respectful crawling with configurable per-source settings
- **Rate Limiting**: Automatic request throttling based on source policies
- **Environment-based Configuration**: Sensitive data stored in environment variables
- **Input Validation**: Comprehensive input sanitization and validation
- **Secure Defaults**: Production-ready security configurations

## ğŸš€ Deployment

### Docker Compose (Recommended)
```bash
# Development
docker-compose -f docker-compose.dev.yml up --build -d

# Production
docker-compose up --build -d
```

### AWS Deployment
See [AWS Deployment Guide](AWS_DEPLOYMENT_README.md) for cloud deployment instructions.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for new functionality
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## âš ï¸ Important Notes

- **Research Purpose**: This tool is designed for legitimate threat intelligence research
- **Respectful Crawling**: Always respect website terms of service and robots.txt policies
- **Rate Limiting**: Built-in rate limiting helps maintain respectful data collection
- **Source Tiering**: Prioritize premium sources while maintaining comprehensive coverage

## ğŸ†˜ Support

For issues, questions, or contributions:
- Create an issue on GitHub
- Check the documentation in `docs/`
- Review the troubleshooting guide in `GITHUB_TROUBLESHOOTING.md`
