{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM Testing Notebook\n",
        "Test different models via web API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing API connection...\n",
            "\u2705 Connected! Article: Velociraptor WSUS Exploitation, Pt. I: WSUS-Up?...\n"
          ]
        }
      ],
      "source": [
        "import httpx\n",
        "import os\n",
        "from datetime import datetime\n",
        "import csv\n",
        "\n",
        "# Function to get article from API\n",
        "def get_article_from_api(article_id):\n",
        "    api_url = f\"http://localhost:8001/api/articles/{article_id}\"\n",
        "    try:\n",
        "        response = httpx.get(api_url, timeout=10.0)\n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        else:\n",
        "            print(f\"\u274c API Error: {response.status_code} - {response.text}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Connection error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Test connection\n",
        "print(\"Testing API connection...\")\n",
        "article = get_article_from_api(68)\n",
        "if article:\n",
        "    print(f\"\u2705 Connected! Article: {article['title'][:50]}...\")\n",
        "else:\n",
        "    print(\"\u274c Connection failed\")\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing LMStudio connection...\n",
            "\u2705 Found 20 models\n",
            "First model: openai/gpt-oss-20b\n"
          ]
        }
      ],
      "source": [
        "# Get available models from LMStudio\n",
        "def get_lmstudio_models():\n",
        "    lmstudio_url = \"http://localhost:1234/v1\"\n",
        "    try:\n",
        "        response = httpx.get(f\"{lmstudio_url}/models\", timeout=5.0)\n",
        "        if response.status_code == 200:\n",
        "            models_data = response.json()\n",
        "            return [m[\"id\"] for m in models_data.get(\"data\", [])]\n",
        "        else:\n",
        "            print(f\"\u274c LMStudio error: {response.status_code}\")\n",
        "            return []\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c LMStudio connection error: {e}\")\n",
        "        return []\n",
        "\n",
        "# Test LMStudio\n",
        "print(\"Testing LMStudio connection...\")\n",
        "models = get_lmstudio_models()\n",
        "print(f\"\u2705 Found {len(models)} models\")\n",
        "if models:\n",
        "    print(f\"First model: {models[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83e\udd16 Testing model: openai/gpt-oss-20b\n",
            "\u2705 SUCCESS!\n",
            "Response: **Extracted Command\u2011Line Statements**\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"cmdlines\": [\n",
            "    \"\\\"C:\\\\Program Files\\\\Velociraptor\\\\Velociraptor.exe\\\" --config \\\"C:\\\\Program Files\\\\Velociraptor\\\\/client.config.yaml\\\" service ru...\n"
          ]
        }
      ],
      "source": [
        "# Test LLM call\n",
        "def test_llm(model, prompt, article):\n",
        "    lmstudio_url = \"http://localhost:1234/v1\"\n",
        "    \n",
        "    full_prompt = f\"\"\"Article Title: {article['title']}\n",
        "Article URL: {article.get('canonical_url', '')}\n",
        "\n",
        "Article Content:\n",
        "{article['content'][:4000]}...\n",
        "\n",
        "Task: {prompt}\"\"\"\n",
        "    \n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": full_prompt}],\n",
        "        \"temperature\": 0.7,\n",
        "        \"max_tokens\": 1000\n",
        "    }\n",
        "    \n",
        "    start = time.perf_counter()\n",
        "    try:\n",
        "        response = httpx.post(f\"{lmstudio_url}/chat/completions\", json=payload, timeout=60.0)\n",
        "        duration = time.perf_counter() - start\n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            content = result[\"choices\"][0][\"message\"][\"content\"]\n",
        "            return {\"status\": \"success\", \"response\": content, \"length\": len(content), \"duration_sec\": duration}\n",
        "        else:\n",
        "            return {\"status\": \"error\", \"response\": f\"API Error: {response.status_code}\", \"length\": 0, \"duration_sec\": duration}\n",
        "    except Exception as e:\n",
        "        duration = time.perf_counter() - start\n",
        "        return {\"status\": \"error\", \"response\": f\"Exception: {str(e)}\", \"length\": 0, \"duration_sec\": duration}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Enhanced LLM Testing\n",
        "Select multiple models, custom articles, and get CSV output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Load default CmdlineExtract prompt from repo files; fall back to a short default if missing.\n",
        "def load_cmdline_prompt():\n",
        "    candidates = [\n",
        "        Path(\"src/prompts/CmdlineExtract\"),\n",
        "        Path(\"prompts/CmdlineExtract\"),\n",
        "    ]\n",
        "    for p in candidates:\n",
        "        if p.exists():\n",
        "            return p.read_text().strip()\n",
        "    return \"\"\"You are a specialized extraction agent focused on extracting explicit Windows command-line observables from threat intelligence articles. Extract only literal Windows command lines with an executable/system utility plus at least one argument/switch/parameter/pipeline/redirection. Respond with a JSON object: {\"cmdline_items\": [...], \"count\": <int>, \"qa_corrections\": {\"removed\": [], \"added\": [], \"summary\": \"None.\"}}. Use double backslashes for Windows paths.\"\"\"\n",
        "\n",
        "# Load CmdLine QA prompt (mirrors workflow QA agent)\n",
        "def load_cmdline_qa_prompt():\n",
        "    candidates = [\n",
        "        Path(\"src/prompts/CmdLineQA\"),\n",
        "        Path(\"prompts/CmdLineQA\"),\n",
        "    ]\n",
        "    for p in candidates:\n",
        "        if p.exists():\n",
        "            return p.read_text().strip()\n",
        "    return \"Review extracted command-lines. If any are invalid or missing, provide added/removed entries in qa_corrections and return the corrected list. Respond with JSON matching {\\\"cmdline_items\\\": [...], \\\"count\\\": <int>, \\\"qa_corrections\\\": {\\\"removed\\\": [], \\\"added\\\": [], \\\"summary\\\": \\\"None.\\\"}}.\"\n",
        "\n",
        "CMDLINE_PROMPT = load_cmdline_prompt()\n",
        "CMDLINE_QA_PROMPT = load_cmdline_qa_prompt()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enhanced testing interface ready!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Global results\n",
        "results_df = pd.DataFrame(columns=[\"timestamp\", \"agent\", \"article_id\", \"article_title\", \"model\", \"prompt\", \"response\", \"response_length\", \"duration_sec\", \"status\"])\n",
        "\n",
        "# UI Widgets\n",
        "article_input = widgets.Text(\n",
        "    value=\"68\",\n",
        "    placeholder=\"Enter article ID\",\n",
        "    description=\"Article ID:\",\n",
        "    layout=widgets.Layout(width=\"200px\")\n",
        ")\n",
        "\n",
        "model_select = widgets.SelectMultiple(\n",
        "    options=models if \"models\" in globals() else [],\n",
        "    description=\"Cmd Models:\",\n",
        "    layout=widgets.Layout(width=\"100%\", height=\"150px\")\n",
        ")\n",
        "\n",
        "prompt_input = widgets.Textarea(\n",
        "    value=CMDLINE_PROMPT,\n",
        "    placeholder=\"Enter your prompt here...\",\n",
        "    description=\"Cmd Prompt:\",\n",
        "    layout=widgets.Layout(width=\"100%\", height=\"120px\")\n",
        ")\n",
        "\n",
        "qa_model_select = widgets.SelectMultiple(\n",
        "    options=models if \"models\" in globals() else [],\n",
        "    description=\"QA Models:\",\n",
        "    layout=widgets.Layout(width=\"100%\", height=\"150px\")\n",
        ")\n",
        "\n",
        "qa_prompt_input = widgets.Textarea(\n",
        "    value=CMDLINE_QA_PROMPT,\n",
        "    placeholder=\"Enter QA prompt...\",\n",
        "    description=\"QA Prompt:\",\n",
        "    layout=widgets.Layout(width=\"100%\", height=\"120px\")\n",
        ")\n",
        "\n",
        "run_button = widgets.Button(\n",
        "    description=\"Run Cmd Tests\",\n",
        "    button_style=\"primary\",\n",
        "    tooltip=\"Run LLM tests with selected parameters\"\n",
        ")\n",
        "\n",
        "qa_run_button = widgets.Button(\n",
        "    description=\"Run QA Tests\",\n",
        "    button_style=\"info\",\n",
        "    tooltip=\"Run QA tests with selected parameters\"\n",
        ")\n",
        "\n",
        "csv_button = widgets.Button(\n",
        "    description=\"Save CSV\",\n",
        "    button_style=\"success\",\n",
        "    tooltip=\"Save results to CSV file\"\n",
        ")\n",
        "\n",
        "output_area = widgets.Output()\n",
        "\n",
        "print(\"Enhanced testing interface ready!\")\n",
        "\n",
        "# Holds latest cmdline extraction outputs for QA chaining\n",
        "last_cmdline_outputs = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Button handlers connected!\n"
          ]
        }
      ],
      "source": [
        "# Enhanced testing functions\n",
        "\n",
        "def run_enhanced_tests(article_id, selected_models, prompt):\n",
        "    global results_df, last_cmdline_outputs\n",
        "    last_cmdline_outputs = []\n",
        "    \n",
        "    print(f\"Getting article {article_id}...\")\n",
        "    article = get_article_from_api(article_id)\n",
        "    if not article:\n",
        "        print(f\"\u274c Article {article_id} not found\")\n",
        "        return\n",
        "    \n",
        "    print(f\"\u2705 Article: {article['title'][:60]}...\")\n",
        "    print(f\"\ud83d\udccf Content: {len(article['content'])} chars\")\n",
        "    print(f\"\ud83e\udd16 Testing {len(selected_models)} models \u00d7 1 prompt (Cmdline Extract)\")\n",
        "    print()\n",
        "    \n",
        "    for i, model in enumerate(selected_models, 1):\n",
        "        model_name = model.split('/')[-1]\n",
        "        print(f\"\ud83e\udde0 Cmd Agent Model {i}/{len(selected_models)}: {model_name}\")\n",
        "        \n",
        "        result = test_llm(model, prompt, article)\n",
        "        \n",
        "        results_df.loc[len(results_df)] = {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"agent\": \"CmdlineExtract\",\n",
        "            \"article_id\": article_id,\n",
        "            \"article_title\": article['title'][:100],\n",
        "            \"model\": model,\n",
        "            \"prompt\": prompt,\n",
        "            \"response\": result['response'],\n",
        "            \"response_length\": result['length'],\n",
        "            \"duration_sec\": result.get('duration_sec', 0.0),\n",
        "            \"status\": result['status']\n",
        "        }\n",
        "        \n",
        "        if result['status'] == 'success':\n",
        "            last_cmdline_outputs.append({\n",
        "                \"model\": model,\n",
        "                \"response\": result['response']\n",
        "            })\n",
        "            print(f\"    \u2705 Success ({result['length']} chars) in {result.get('duration_sec',0):.2f}s\")\n",
        "        else:\n",
        "            print(f\"    \u274c {result['response']}\")\n",
        "    \n",
        "    print(f\"\ud83d\udcca Completed {len(selected_models)} Cmd tests!\")\n",
        "    print(f\"Results stored in DataFrame ({len(results_df)} total rows)\")\n",
        "\n",
        "\n",
        "def run_cmdline_qa_tests(article_id, selected_models, prompt):\n",
        "    global results_df, last_cmdline_outputs\n",
        "    \n",
        "    print(f\"Getting article {article_id} for QA...\")\n",
        "    article = get_article_from_api(article_id)\n",
        "    if not article:\n",
        "        print(f\"\u274c Article {article_id} not found\")\n",
        "        return\n",
        "    \n",
        "    if not last_cmdline_outputs:\n",
        "        print(\"\u26a0\ufe0f No extractor outputs available. Run Cmdline Extract tests first.\")\n",
        "    \n",
        "    extraction_context = \"\\n\\n\".join(\n",
        "        [f\"Model: {item['model']}\\nOutput:\\n{item['response']}\" for item in last_cmdline_outputs]\n",
        "    ) if last_cmdline_outputs else \"No extractor outputs captured.\"\n",
        "    \n",
        "    print(f\"\ud83e\udd16 Testing {len(selected_models)} models \u00d7 1 prompt (CmdLine QA)\")\n",
        "    print()\n",
        "    \n",
        "    for i, model in enumerate(selected_models, 1):\n",
        "        model_name = model.split('/')[-1]\n",
        "        print(f\"\ud83e\udde0 QA Agent Model {i}/{len(selected_models)}: {model_name}\")\n",
        "        \n",
        "        qa_prompt_with_context = f\"{prompt}\\n\\nExtracted command-lines to evaluate:\\n{extraction_context}\"\n",
        "        result = None\n",
        "        max_attempts = 2\n",
        "        for attempt in range(1, max_attempts + 1):\n",
        "            result = test_llm(model, qa_prompt_with_context, article)\n",
        "            if result.get('status') == 'success' and result.get('response', '').strip():\n",
        "                break\n",
        "            if attempt < max_attempts:\n",
        "                print(f\"    \u26a0\ufe0f QA attempt {attempt} returned empty/failed (status={result.get('status')}, len={len(result.get('response',''))}); retrying...\")\n",
        "        \n",
        "        results_df.loc[len(results_df)] = {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"agent\": \"CmdLineQA\",\n",
        "            \"article_id\": article_id,\n",
        "            \"article_title\": article['title'][:100],\n",
        "            \"model\": model,\n",
        "            \"prompt\": qa_prompt_with_context,\n",
        "            \"response\": result.get('response', ''),\n",
        "            \"response_length\": result.get('length', 0),\n",
        "            \"duration_sec\": result.get('duration_sec', 0.0),\n",
        "            \"status\": result.get('status')\n",
        "        }\n",
        "        \n",
        "        if result.get('status') == 'success':\n",
        "            print(f\"    \u2705 QA Success ({result.get('length',0)} chars) in {result.get('duration_sec',0):.2f}s\")\n",
        "        else:\n",
        "            print(f\"    \u274c {result.get('response','')}\")\n",
        "    \n",
        "    print(f\"\ud83d\udcca Completed {len(selected_models)} QA tests!\")\n",
        "    print(f\"Results stored in DataFrame ({len(results_df)} total rows)\")\n",
        "\n",
        "# Button handlers\n",
        "def on_run_clicked(b):\n",
        "    with output_area:\n",
        "        clear_output()\n",
        "        article_id = article_input.value.strip()\n",
        "        selected_models = list(model_select.value)\n",
        "        prompt = prompt_input.value.strip()\n",
        "        \n",
        "        if not article_id:\n",
        "            print(\"\u274c Please enter an article ID\")\n",
        "            return\n",
        "        if not selected_models:\n",
        "            print(\"\u274c Please select at least one model\")\n",
        "            return\n",
        "        if not prompt:\n",
        "            print(\"\u274c Please enter a prompt\")\n",
        "            return\n",
        "        \n",
        "        print(\"\ud83d\ude80 Starting Cmdline Extract tests...\")\n",
        "        run_enhanced_tests(article_id, selected_models, prompt)\n",
        "\n",
        "\n",
        "def on_qa_run_clicked(b):\n",
        "    with output_area:\n",
        "        clear_output()\n",
        "        article_id = article_input.value.strip()\n",
        "        selected_models = list(qa_model_select.value)\n",
        "        prompt = qa_prompt_input.value.strip()\n",
        "        \n",
        "        if not article_id:\n",
        "            print(\"\u274c Please enter an article ID\")\n",
        "            return\n",
        "        if not selected_models:\n",
        "            print(\"\u274c Please select at least one QA model\")\n",
        "            return\n",
        "        if not prompt:\n",
        "            print(\"\u274c Please enter a QA prompt\")\n",
        "            return\n",
        "        \n",
        "        print(\"\ud83d\ude80 Starting CmdLine QA tests...\")\n",
        "        run_cmdline_qa_tests(article_id, selected_models, prompt)\n",
        "\n",
        "\n",
        "def on_csv_clicked(b):\n",
        "    with output_area:\n",
        "        if results_df.empty:\n",
        "            print(\"\u274c No results to save\")\n",
        "            return\n",
        "        \n",
        "        filename = f\"llm_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "        results_df.to_csv(filename, index=False)\n",
        "        print(f\"\u2705 Saved {len(results_df)} results to {filename}\")\n",
        "\n",
        "# Connect handlers\n",
        "run_button.on_click(on_run_clicked)\n",
        "qa_run_button.on_click(on_qa_run_clicked)\n",
        "csv_button.on_click(on_csv_clicked)\n",
        "\n",
        "print(\"Button handlers connected!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc5f4ba1b0374ff0b049dd40eafe5635",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<h3>\ud83d\ude80 Enhanced LLM Testing</h3>'), Text(value='68', description='Article ID:', layo\u2026"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enhanced UI ready! Select your options and click Run Tests.\n"
          ]
        }
      ],
      "source": [
        "# Display enhanced UI\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML(\"<h3>\ud83d\ude80 Enhanced LLM Testing</h3>\"),\n",
        "    article_input,\n",
        "    widgets.HTML(\"<strong>Cmdline Extract</strong>\"),\n",
        "    model_select,\n",
        "    prompt_input,\n",
        "    widgets.HBox([run_button, csv_button]),\n",
        "    widgets.HTML(\"<strong>CmdLine QA</strong>\"),\n",
        "    qa_model_select,\n",
        "    qa_prompt_input,\n",
        "    qa_run_button,\n",
        "    output_area\n",
        "]))\n",
        "\n",
        "print(\"Enhanced UI ready! Select your options and click Run Tests.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View current results\n",
        "if not results_df.empty:\n",
        "    print(f\"\ud83d\udcca Current results: {len(results_df)} tests\")\n",
        "    display(results_df)\n",
        "else:\n",
        "    print(\"No results yet. Run some tests first!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}