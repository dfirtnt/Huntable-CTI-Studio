{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CommandLine Observables Testing\n",
    "\n",
    "Test different models (LLMs and embedding models) for counting CommandLine observables in CTI articles.\n",
    "\n",
    "**Scope**: Only CommandLinePatterns category\n",
    "\n",
    "**Model Types**:\n",
    "- LLM models: LMStudio, Anthropic, OpenAI\n",
    "- Embedding models: CTI-BERT (pattern-based extraction with semantic validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import httpx\n",
    "\n",
    "# Set project root - use absolute path\n",
    "# Update this path if your project is located elsewhere\n",
    "PROJECT_ROOT = Path('/Users/starlord/CTIScraper')\n",
    "\n",
    "# Alternative: Auto-detect from current notebook location\n",
    "# This assumes notebook is in notebooks/ subdirectory\n",
    "if not PROJECT_ROOT.exists():\n",
    "    # Try to find project root by going up from notebooks/\n",
    "    current = Path.cwd()\n",
    "    if current.name == 'notebooks':\n",
    "        PROJECT_ROOT = current.parent\n",
    "    else:\n",
    "        # Search up for src/database\n",
    "        search_path = current\n",
    "        for _ in range(5):\n",
    "            if (search_path / 'src' / 'database').exists():\n",
    "                PROJECT_ROOT = search_path\n",
    "                break\n",
    "            search_path = search_path.parent\n",
    "        else:\n",
    "            PROJECT_ROOT = Path('/Users/starlord/CTIScraper')  # Fallback\n",
    "\n",
    "# Change to project root directory\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "# Add to Python path\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Now import\n",
    "from src.database.manager import DatabaseManager\n",
    "from src.database.models import ArticleTable\n",
    "from src.utils.content_filter import ContentFilter\n",
    "from src.services.llm_service import LLMService\n",
    "\n",
    "print(f\"✅ Successfully imported modules\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"src exists: {(PROJECT_ROOT / 'src').exists()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the setup worked\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Current working directory: {Path.cwd()}\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"sys.path[0]: {sys.path[0] if sys.path else 'empty'}\")\n",
    "print(f\"src exists: {(PROJECT_ROOT / 'src').exists()}\")\n",
    "print(f\"src/database exists: {(PROJECT_ROOT / 'src' / 'database').exists()}\")\n",
    "\n",
    "# Try importing again to verify\n",
    "try:\n",
    "    from src.database.manager import DatabaseManager\n",
    "    print(\"✅ DatabaseManager imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import failed: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(f\"  - Check if {PROJECT_ROOT / 'src'} exists\")\n",
    "    print(f\"  - Check if {PROJECT_ROOT / 'src' / 'database'} exists\")\n",
    "    print(f\"  - Current working directory: {Path.cwd()}\")\n",
    "    print(f\"  - Try running: cd {PROJECT_ROOT} && jupyter notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Model configurations\n",
    "LLM_MODELS = {\n",
    "    # LMStudio models\n",
    "    'deepseek-r1-qwen3-8b': {\n",
    "        'model_name': 'deepseek/deepseek-r1-0528-qwen3-8b',\n",
    "        'provider': 'lmstudio',\n",
    "        'description': 'DeepSeek R1 Qwen3 8B (reasoning)'\n",
    "    },\n",
    "    'mistral-7b': {\n",
    "        'model_name': 'mistralai/mistral-7b-instruct-v0.3',\n",
    "        'provider': 'lmstudio',\n",
    "        'description': 'Mistral 7B Instruct'\n",
    "    },\n",
    "    'qwen2-7b': {\n",
    "        'model_name': 'qwen2-7b-instruct',\n",
    "        'provider': 'lmstudio',\n",
    "        'description': 'Qwen2 7B Instruct'\n",
    "    },\n",
    "    'llama-3.1-8b': {\n",
    "        'model_name': 'meta-llama-3.1-8b-instruct',\n",
    "        'provider': 'lmstudio',\n",
    "        'description': 'Llama 3.1 8B Instruct'\n",
    "    },\n",
    "    'granite-4-h-tiny': {\n",
    "        'model_name': 'bm/granite-4-h-tiny',\n",
    "        'provider': 'lmstudio',\n",
    "        'description': 'Granite 4H Tiny'\n",
    "    },\n",
    "    \n",
    "    # Anthropic models\n",
    "    'claude-sonnet-4-5': {\n",
    "        'model_name': 'claude-sonnet-4-5',\n",
    "        'provider': 'anthropic',\n",
    "        'description': 'Claude Sonnet 4.5'\n",
    "    },\n",
    "    'claude-haiku-4-5': {\n",
    "        'model_name': 'claude-haiku-4-5',\n",
    "        'provider': 'anthropic',\n",
    "        'description': 'Claude Haiku 4.5'\n",
    "    },\n",
    "    \n",
    "    # OpenAI models\n",
    "    'gpt-4o-mini': {\n",
    "        'model_name': 'gpt-4o-mini',\n",
    "        'provider': 'openai',\n",
    "        'description': 'GPT-4o Mini'\n",
    "    },\n",
    "    'gpt-5-mini': {\n",
    "        'model_name': 'gpt-5-mini',\n",
    "        'provider': 'openai',\n",
    "        'description': 'GPT-5 Mini'\n",
    "    },\n",
    "    'gpt-5.1': {\n",
    "        'model_name': 'gpt-5.1',\n",
    "        'provider': 'openai',\n",
    "        'description': 'GPT-5.1'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Embedding model configurations\n",
    "EMBEDDING_MODELS = {\n",
    "    'cti-bert': {\n",
    "        'model_name': 'ibm-research/CTI-BERT',\n",
    "        'description': 'CTI-BERT (pattern + embedding validation)'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Test parameters\n",
    "TEMPERATURE = 0.0\n",
    "SEED = 42\n",
    "JUNK_FILTER_THRESHOLD = 0.8\n",
    "\n",
    "print(f\"Available LLM models: {len(LLM_MODELS)}\")\n",
    "print(f\"Available embedding models: {len(EMBEDDING_MODELS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Select Articles to Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Select by article IDs\n",
    "ARTICLE_IDS = [1937, 1909, 1866, 1860, 1794]  # Modify this list\n",
    "\n",
    "# Option 2: Select by URLs (uncomment and modify)\n",
    "# ARTICLE_URLS = [\n",
    "#     \"https://thedfirreport.com/2025/08/05/from-bing-search-to-ransomware-bumblebee-and-adaptixc2-deliver-akira/\",\n",
    "#     # Add more URLs here\n",
    "# ]\n",
    "\n",
    "# Load articles from database\n",
    "db_manager = DatabaseManager()\n",
    "db_session = db_manager.get_session()\n",
    "\n",
    "articles = []\n",
    "try:\n",
    "    if 'ARTICLE_IDS' in locals() and ARTICLE_IDS:\n",
    "        for article_id in ARTICLE_IDS:\n",
    "            article = db_session.query(ArticleTable).filter(ArticleTable.id == article_id).first()\n",
    "            if article:\n",
    "                articles.append({\n",
    "                    'id': article.id,\n",
    "                    'title': article.title,\n",
    "                    'url': article.canonical_url,\n",
    "                    'content': article.content or \"\"\n",
    "                })\n",
    "    elif 'ARTICLE_URLS' in locals() and ARTICLE_URLS:\n",
    "        for url in ARTICLE_URLS:\n",
    "            article = db_session.query(ArticleTable).filter(ArticleTable.canonical_url == url).first()\n",
    "            if article:\n",
    "                articles.append({\n",
    "                    'id': article.id,\n",
    "                    'title': article.title,\n",
    "                    'url': article.canonical_url,\n",
    "                    'content': article.content or \"\"\n",
    "                })\n",
    "finally:\n",
    "    db_session.close()\n",
    "\n",
    "print(f\"Loaded {len(articles)} articles:\")\n",
    "for article in articles:\n",
    "    print(f\"  [{article['id']}] {article['title'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Select Models to Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which LLM models to test (modify this list)\n",
    "LLM_MODELS_TO_TEST = [\n",
    "    'gpt-4o-mini',\n",
    "    'claude-sonnet-4-5',\n",
    "    'deepseek-r1-qwen3-8b',\n",
    "    'mistral-7b',\n",
    "    # Add more model keys from LLM_MODELS dict above\n",
    "]\n",
    "\n",
    "# Select which embedding models to test (modify this list)\n",
    "EMBEDDING_MODELS_TO_TEST = [\n",
    "    'cti-bert',\n",
    "    # Add more embedding models if available\n",
    "]\n",
    "\n",
    "# Validate models\n",
    "invalid_llm = [m for m in LLM_MODELS_TO_TEST if m not in LLM_MODELS]\n",
    "invalid_embedding = [m for m in EMBEDDING_MODELS_TO_TEST if m not in EMBEDDING_MODELS]\n",
    "\n",
    "if invalid_llm:\n",
    "    print(f\"⚠️  Invalid LLM models: {invalid_llm}\")\n",
    "    print(f\"Available LLM models: {list(LLM_MODELS.keys())}\")\n",
    "if invalid_embedding:\n",
    "    print(f\"⚠️  Invalid embedding models: {invalid_embedding}\")\n",
    "    print(f\"Available embedding models: {list(EMBEDDING_MODELS.keys())}\")\n",
    "\n",
    "if not invalid_llm and not invalid_embedding:\n",
    "    print(f\"✅ Testing {len(LLM_MODELS_TO_TEST)} LLM models and {len(EMBEDDING_MODELS_TO_TEST)} embedding models:\")\n",
    "    for model_key in LLM_MODELS_TO_TEST:\n",
    "        config = LLM_MODELS[model_key]\n",
    "        print(f\"  LLM: {model_key} - {config['description']} ({config['provider']})\")\n",
    "    for model_key in EMBEDDING_MODELS_TO_TEST:\n",
    "        config = EMBEDDING_MODELS[model_key]\n",
    "        print(f\"  Embedding: {model_key} - {config['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Functions\n",
    "\n",
    "The functions are available in `scripts/test_commandline_observables_standalone.py`.\n",
    "You can either:\n",
    "1. Import them from the script\n",
    "2. Copy the function code into cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Import from standalone script\n",
    "# Uncomment to use:\n",
    "# import sys\n",
    "# sys.path.insert(0, str(PROJECT_ROOT / 'scripts'))\n",
    "# from test_commandline_observables_standalone import (\n",
    "#     count_commandline_with_llm,\n",
    "#     count_commandline_with_ctibert\n",
    "# )\n",
    "\n",
    "# Option 2: Copy functions from standalone script into cells below\n",
    "print(\"Functions should be defined in cells below or imported from standalone script\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LLM-Based CommandLine Counting Function\n",
    "\n",
    "Copy the `count_commandline_with_llm` function from `scripts/test_commandline_observables_standalone.py` into this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def count_commandline_with_llm(\n",
    "    article_content: str,\n",
    "    model_key: str,\n",
    "    temperature: float = 0.0,\n",
    "    seed: int = 42,\n",
    "    junk_filter_threshold: float = 0.8,\n",
    "    article_id: Optional[int] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Count CommandLine observables using specified LLM model.\"\"\"\n",
    "    \n",
    "    model_config = LLM_MODELS[model_key]\n",
    "    model_name = model_config['model_name']\n",
    "    provider = model_config['provider']\n",
    "    \n",
    "    # Apply junk filter\n",
    "    content_filter = ContentFilter()\n",
    "    hunt_score = 0\n",
    "    \n",
    "    filter_result = content_filter.filter_content(\n",
    "        article_content,\n",
    "        min_confidence=junk_filter_threshold,\n",
    "        hunt_score=hunt_score,\n",
    "        article_id=article_id\n",
    "    )\n",
    "    filtered_content = filter_result.filtered_content or article_content\n",
    "    \n",
    "    # Build focused prompt for CommandLine only\n",
    "    system_content = \"\"\"You are an expert at identifying command-line observables in threat intelligence articles.\n",
    "\n",
    "Your task: Count ONLY command-line patterns (executed command strings, flags, parameters) in the provided article.\n",
    "\n",
    "Command-line patterns include:\n",
    "- Executed command strings (e.g., \"powershell.exe -enc\", \"cmd /c whoami\")\n",
    "- Command flags and parameters\n",
    "- Script execution commands\n",
    "- Shell commands\n",
    "\n",
    "Do NOT count:\n",
    "- Process names alone (without command-line)\n",
    "- File paths (unless part of a command)\n",
    "- Registry keys\n",
    "- Network connections\n",
    "- Other observables\n",
    "\n",
    "Output ONLY valid JSON in this exact format:\n",
    "{\"CommandLinePatterns\": <integer>, \"Total\": <integer>}\n",
    "\n",
    "CRITICAL: Output ONLY the JSON object. No markdown, no explanations, no other text.\"\"\"\n",
    "    \n",
    "    user_content = f\"Article:\\n\\n{filtered_content}\\n\\nCount command-line observables and output JSON.\"\n",
    "    \n",
    "    # Call appropriate API based on provider\n",
    "    response_text = ''\n",
    "    reasoning_content = ''\n",
    "    usage_info = {}\n",
    "    error = None\n",
    "    \n",
    "    try:\n",
    "        if provider == 'openai':\n",
    "            openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "            if not openai_api_key:\n",
    "                return {'error': 'OpenAI API key not configured', 'count': None, 'parse_success': False}\n",
    "            \n",
    "            is_gpt5 = model_name.startswith('gpt-5')\n",
    "            token_param = \"max_completion_tokens\" if is_gpt5 else \"max_tokens\"\n",
    "            \n",
    "            async with httpx.AsyncClient(timeout=300.0) as client:\n",
    "                payload = {\n",
    "                    \"model\": model_name,\n",
    "                    token_param: 2000,\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": system_content},\n",
    "                        {\"role\": \"user\", \"content\": user_content}\n",
    "                    ]\n",
    "                }\n",
    "                if model_name != 'gpt-5-mini':\n",
    "                    payload[\"temperature\"] = temperature\n",
    "                if seed is not None:\n",
    "                    payload[\"seed\"] = seed\n",
    "                \n",
    "                response = await client.post(\n",
    "                    \"https://api.openai.com/v1/chat/completions\",\n",
    "                    headers={\n",
    "                        \"Authorization\": f\"Bearer {openai_api_key}\",\n",
    "                        \"Content-Type\": \"application/json\"\n",
    "                    },\n",
    "                    json=payload\n",
    "                )\n",
    "                \n",
    "                if response.status_code != 200:\n",
    "                    error_detail = response.json().get('error', {}).get('message', response.text)\n",
    "                    return {'error': f\"OpenAI API error: {error_detail}\", 'count': None, 'parse_success': False}\n",
    "                \n",
    "                result = response.json()\n",
    "                response_text = result.get('choices', [{}])[0].get('message', {}).get('content', '')\n",
    "                usage_info = result.get('usage', {})\n",
    "                \n",
    "        elif provider == 'anthropic':\n",
    "            anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "            if not anthropic_api_key:\n",
    "                return {'error': 'Anthropic API key not configured', 'count': None, 'parse_success': False}\n",
    "            \n",
    "            async with httpx.AsyncClient(timeout=300.0) as client:\n",
    "                response = await client.post(\n",
    "                    \"https://api.anthropic.com/v1/messages\",\n",
    "                    headers={\n",
    "                        \"x-api-key\": anthropic_api_key,\n",
    "                        \"Content-Type\": \"application/json\",\n",
    "                        \"anthropic-version\": \"2023-06-01\"\n",
    "                    },\n",
    "                    json={\n",
    "                        \"model\": model_name,\n",
    "                        \"max_tokens\": 2000,\n",
    "                        \"temperature\": temperature,\n",
    "                        \"system\": system_content,\n",
    "                        \"messages\": [\n",
    "                            {\"role\": \"user\", \"content\": user_content}\n",
    "                        ]\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                if response.status_code != 200:\n",
    "                    return {'error': f\"Anthropic API error: {response.text}\", 'count': None, 'parse_success': False}\n",
    "                \n",
    "                result = response.json()\n",
    "                response_text = result.get('content', [{}])[0].get('text', '')\n",
    "                usage_info = result.get('usage', {})\n",
    "                \n",
    "        elif provider == 'lmstudio':\n",
    "            original_extract = os.getenv(\"LMSTUDIO_MODEL_EXTRACT\")\n",
    "            \n",
    "            try:\n",
    "                os.environ[\"LMSTUDIO_MODEL_EXTRACT\"] = model_name\n",
    "                \n",
    "                llm_service = LLMService()\n",
    "                llm_service.temperature = temperature\n",
    "                llm_service.seed = seed\n",
    "                \n",
    "                actual_model_name = llm_service.model_extract\n",
    "                \n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": system_content},\n",
    "                    {\"role\": \"user\", \"content\": user_content}\n",
    "                ]\n",
    "                \n",
    "                messages = llm_service._convert_messages_for_model(messages, actual_model_name)\n",
    "                \n",
    "                is_reasoning_model = 'r1' in actual_model_name.lower() or 'reasoning' in actual_model_name.lower()\n",
    "                max_tokens = 2000 if is_reasoning_model else 1500\n",
    "                \n",
    "                payload = {\n",
    "                    \"model\": actual_model_name,\n",
    "                    \"messages\": messages,\n",
    "                    \"max_tokens\": max_tokens,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"top_p\": llm_service.top_p,\n",
    "                }\n",
    "                \n",
    "                if seed is not None:\n",
    "                    payload[\"seed\"] = seed\n",
    "                \n",
    "                result = await llm_service._post_lmstudio_chat(\n",
    "                    payload,\n",
    "                    model_name=actual_model_name,\n",
    "                    timeout=300.0,\n",
    "                    failure_context=\"CommandLine counting\"\n",
    "                )\n",
    "                \n",
    "                choice = result.get('choices', [{}])[0]\n",
    "                message = choice.get('message', {})\n",
    "                reasoning_content = message.get('reasoning_content', '')\n",
    "                response_text = message.get('content', '')\n",
    "                usage_info = result.get('usage', {})\n",
    "                \n",
    "            finally:\n",
    "                if original_extract:\n",
    "                    os.environ[\"LMSTUDIO_MODEL_EXTRACT\"] = original_extract\n",
    "                elif \"LMSTUDIO_MODEL_EXTRACT\" in os.environ:\n",
    "                    del os.environ[\"LMSTUDIO_MODEL_EXTRACT\"]\n",
    "        \n",
    "        # Extract JSON from reasoning if needed\n",
    "        if reasoning_content and not response_text:\n",
    "            if '{' in reasoning_content:\n",
    "                last_brace = reasoning_content.rfind('}')\n",
    "                if last_brace > 0:\n",
    "                    json_start = reasoning_content.rfind('{', 0, last_brace)\n",
    "                    if json_start >= 0:\n",
    "                        potential_json = reasoning_content[json_start:last_brace+1]\n",
    "                        try:\n",
    "                            test_parse = json.loads(potential_json)\n",
    "                            if 'CommandLinePatterns' in test_parse:\n",
    "                                response_text = potential_json\n",
    "                        except:\n",
    "                            pass\n",
    "        \n",
    "        # Parse JSON\n",
    "        count = None\n",
    "        parse_success = False\n",
    "        \n",
    "        if response_text:\n",
    "            try:\n",
    "                # Extract JSON from markdown if present\n",
    "                json_text = response_text\n",
    "                if \"```json\" in response_text:\n",
    "                    json_start = response_text.find(\"```json\") + 7\n",
    "                    json_end = response_text.find(\"```\", json_start)\n",
    "                    json_text = response_text[json_start:json_end].strip()\n",
    "                elif \"```\" in response_text:\n",
    "                    json_start = response_text.find(\"```\") + 3\n",
    "                    json_end = response_text.find(\"```\", json_start)\n",
    "                    json_text = response_text[json_start:json_end].strip()\n",
    "                else:\n",
    "                    # Find first { and last }\n",
    "                    first_brace = json_text.find('{')\n",
    "                    last_brace = json_text.rfind('}')\n",
    "                    if first_brace >= 0 and last_brace > first_brace:\n",
    "                        json_text = json_text[first_brace:last_brace+1]\n",
    "                \n",
    "                parsed = json.loads(json_text)\n",
    "                count = parsed.get('CommandLinePatterns', 0)\n",
    "                if isinstance(count, str) and count.isdigit():\n",
    "                    count = int(count)\n",
    "                parse_success = True\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                error = f\"JSON parse error: {str(e)}\"\n",
    "            except Exception as e:\n",
    "                error = f\"Parse error: {str(e)}\"\n",
    "        \n",
    "        return {\n",
    "            'count': count,\n",
    "            'parse_success': parse_success,\n",
    "            'raw_response': response_text,\n",
    "            'reasoning_content': reasoning_content if reasoning_content else None,\n",
    "            'usage': usage_info,\n",
    "            'error': error\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        return {\n",
    "            'error': str(e),\n",
    "            'count': None,\n",
    "            'parse_success': False,\n",
    "            'traceback': traceback.format_exc()\n",
    "        }\n",
    "\n",
    "# ============================================================================\n",
    "# CTI-BERT PATTERN-BASED COUNTING FUNCTION\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. CTI-BERT Pattern-Based CommandLine Counting Function\n",
    "\n",
    "Copy the `count_commandline_with_ctibert` function from `scripts/test_commandline_observables_standalone.py` into this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_commandline_with_ctibert(\n",
    "    article_content: str,\n",
    "    model_key: str = 'cti-bert',\n",
    "    junk_filter_threshold: float = 0.8,\n",
    "    article_id: Optional[int] = None,\n",
    "    use_embedding_validation: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Count CommandLine observables using CTI-BERT pattern matching + optional embedding validation.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        from src.utils.ctibert_ner_extractor import CTIBERTNERExtractor\n",
    "        import torch\n",
    "    except ImportError as e:\n",
    "        return {\n",
    "            'error': f'CTI-BERT import failed: {str(e)}',\n",
    "            'count': None,\n",
    "            'parse_success': False\n",
    "        }\n",
    "    \n",
    "    # Apply junk filter\n",
    "    content_filter = ContentFilter()\n",
    "    hunt_score = 0\n",
    "    \n",
    "    filter_result = content_filter.filter_content(\n",
    "        article_content,\n",
    "        min_confidence=junk_filter_threshold,\n",
    "        hunt_score=hunt_score,\n",
    "        article_id=article_id\n",
    "    )\n",
    "    filtered_content = filter_result.filtered_content or article_content\n",
    "    \n",
    "    # Command-line pattern regexes\n",
    "    command_patterns = [\n",
    "        # PowerShell commands\n",
    "        r'powershell\\.exe\\s+(?:-enc|-command|-c|-encodedcommand|-e|-executionpolicy|\\s+[^\\s]+)',\n",
    "        r'pwsh\\s+(?:-enc|-command|-c|-encodedcommand|-e|-executionpolicy|\\s+[^\\s]+)',\n",
    "        \n",
    "        # CMD commands\n",
    "        r'cmd\\.exe\\s+/[cC]\\s+[^\\n]+',\n",
    "        r'cmd\\s+/[cC]\\s+[^\\n]+',\n",
    "        \n",
    "        # Bash/Shell commands\n",
    "        r'bash\\s+-c\\s+[^\\n]+',\n",
    "        r'sh\\s+-c\\s+[^\\n]+',\n",
    "        \n",
    "        # Generic executable with flags\n",
    "        r'\\b[a-zA-Z0-9_\\-]+\\.exe\\s+(?:-[a-zA-Z]+|/[a-zA-Z]+|\\s+[^\\s]+){1,}',\n",
    "        \n",
    "        # Script execution\n",
    "        r'python\\s+[^\\s]+\\.py\\s+[^\\n]+',\n",
    "        r'python3\\s+[^\\s]+\\.py\\s+[^\\n]+',\n",
    "        r'perl\\s+[^\\s]+\\.pl\\s+[^\\n]+',\n",
    "        r'ruby\\s+[^\\s]+\\.rb\\s+[^\\n]+',\n",
    "        \n",
    "        # Common command patterns\n",
    "        r'\\b(?:wmic|reg|schtasks|sc|net|netsh|bcdedit|dism|sfc)\\s+[^\\n]+',\n",
    "        \n",
    "        # Base64 encoded commands\n",
    "        r'powershell.*-enc\\s+[A-Za-z0-9+/=]{20,}',\n",
    "    ]\n",
    "    \n",
    "    # Find all matches\n",
    "    all_matches = []\n",
    "    for pattern in command_patterns:\n",
    "        matches = re.finditer(pattern, filtered_content, re.IGNORECASE | re.MULTILINE)\n",
    "        for match in matches:\n",
    "            all_matches.append(match.group(0).strip())\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    unique_commands = []\n",
    "    seen = set()\n",
    "    for cmd in all_matches:\n",
    "        normalized = re.sub(r'\\s+', ' ', cmd)\n",
    "        if normalized not in seen and len(normalized) > 5:\n",
    "            seen.add(normalized)\n",
    "            unique_commands.append(normalized)\n",
    "    \n",
    "    count = len(unique_commands)\n",
    "    \n",
    "    # Optional: Use CTI-BERT embeddings to validate commands\n",
    "    validated_count = count\n",
    "    confidence = 1.0\n",
    "    \n",
    "    if use_embedding_validation and count > 0:\n",
    "        try:\n",
    "            extractor = CTIBERTNERExtractor(use_gpu=True)\n",
    "            \n",
    "            # Generate embedding for article content (for context)\n",
    "            content_sample = filtered_content[:1000]\n",
    "            content_embedding = extractor._get_embedding(content_sample)\n",
    "            \n",
    "            # Generate embedding for cybersecurity context\n",
    "            cti_context_embedding = extractor._get_embedding(\"cybersecurity threat intelligence malware attack command execution\")\n",
    "            \n",
    "            # Calculate similarity\n",
    "            similarity = torch.nn.functional.cosine_similarity(\n",
    "                content_embedding.unsqueeze(0),\n",
    "                cti_context_embedding.unsqueeze(0)\n",
    "            ).item()\n",
    "            \n",
    "            # Adjust confidence based on similarity\n",
    "            if similarity < 0.3:\n",
    "                confidence = 0.7\n",
    "            elif similarity < 0.5:\n",
    "                confidence = 0.85\n",
    "            else:\n",
    "                confidence = 0.95\n",
    "            \n",
    "        except Exception as e:\n",
    "            confidence = 0.8\n",
    "    \n",
    "    return {\n",
    "        'count': count,\n",
    "        'validated_count': validated_count,\n",
    "        'confidence': confidence,\n",
    "        'parse_success': True,\n",
    "        'raw_matches': unique_commands[:10],\n",
    "        'total_matches_found': len(unique_commands),\n",
    "        'error': None\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tests for all article-model combinations\n",
    "results = []\n",
    "\n",
    "# Test LLM models\n",
    "for article in articles:\n",
    "    for model_key in LLM_MODELS_TO_TEST:\n",
    "        print(f\"Testing Article {article['id']} with LLM {model_key}...\", end=\" \")\n",
    "        \n",
    "        result = await count_commandline_with_llm(\n",
    "            article_content=article['content'],\n",
    "            model_key=model_key,\n",
    "            temperature=TEMPERATURE,\n",
    "            seed=SEED,\n",
    "            junk_filter_threshold=JUNK_FILTER_THRESHOLD,\n",
    "            article_id=article['id']\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'article_id': article['id'],\n",
    "            'article_title': article['title'],\n",
    "            'article_url': article['url'],\n",
    "            'model_key': model_key,\n",
    "            'model_description': LLM_MODELS[model_key]['description'],\n",
    "            'model_type': 'llm',\n",
    "            'provider': LLM_MODELS[model_key]['provider'],\n",
    "            'commandline_count': result.get('count'),\n",
    "            'parse_success': result.get('parse_success', False),\n",
    "            'error': result.get('error'),\n",
    "            'raw_response': result.get('raw_response', '')[:200],\n",
    "            'usage': result.get('usage', {})\n",
    "        })\n",
    "        \n",
    "        status = \"✅\" if result.get('parse_success') else \"❌\"\n",
    "        count = result.get('count', 'N/A')\n",
    "        print(f\"{status} Count: {count}\")\n",
    "\n",
    "# Test embedding models\n",
    "for article in articles:\n",
    "    for model_key in EMBEDDING_MODELS_TO_TEST:\n",
    "        print(f\"Testing Article {article['id']} with Embedding {model_key}...\", end=\" \")\n",
    "        \n",
    "        result = count_commandline_with_ctibert(\n",
    "            article_content=article['content'],\n",
    "            model_key=model_key,\n",
    "            junk_filter_threshold=JUNK_FILTER_THRESHOLD,\n",
    "            article_id=article['id'],\n",
    "            use_embedding_validation=True\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'article_id': article['id'],\n",
    "            'article_title': article['title'],\n",
    "            'article_url': article['url'],\n",
    "            'model_key': model_key,\n",
    "            'model_description': EMBEDDING_MODELS[model_key]['description'],\n",
    "            'model_type': 'embedding',\n",
    "            'provider': 'cti-bert',\n",
    "            'commandline_count': result.get('count'),\n",
    "            'validated_count': result.get('validated_count'),\n",
    "            'confidence': result.get('confidence'),\n",
    "            'parse_success': result.get('parse_success', False),\n",
    "            'error': result.get('error'),\n",
    "            'raw_matches': result.get('raw_matches', []),\n",
    "            'usage': {}\n",
    "        })\n",
    "        \n",
    "        status = \"✅\" if result.get('parse_success') else \"❌\"\n",
    "        count = result.get('count', 'N/A')\n",
    "        confidence = result.get('confidence', 'N/A')\n",
    "        print(f\"{status} Count: {count} (confidence: {confidence})\")\n",
    "\n",
    "print(f\"\\n✅ Completed {len(results)} tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for easy viewing\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Separate LLM and embedding results\n",
    "llm_df = df[df['model_type'] == 'llm'].copy()\n",
    "embedding_df = df[df['model_type'] == 'embedding'].copy()\n",
    "\n",
    "# Pivot table for LLM models\n",
    "if len(llm_df) > 0:\n",
    "    llm_pivot = llm_df.pivot_table(\n",
    "        index=['article_id', 'article_title'],\n",
    "        columns='model_description',\n",
    "        values='commandline_count',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"LLM MODELS - COMMANDLINE COUNTS BY ARTICLE\")\n",
    "    print(\"=\"*80)\n",
    "    display(llm_pivot)\n",
    "\n",
    "# Pivot table for embedding models\n",
    "if len(embedding_df) > 0:\n",
    "    embedding_pivot = embedding_df.pivot_table(\n",
    "        index=['article_id', 'article_title'],\n",
    "        columns='model_description',\n",
    "        values='commandline_count',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EMBEDDING MODELS - COMMANDLINE COUNTS BY ARTICLE\")\n",
    "    print(\"=\"*80)\n",
    "    display(embedding_pivot)\n",
    "\n",
    "# Combined comparison\n",
    "if len(llm_df) > 0 and len(embedding_df) > 0:\n",
    "    combined_pivot = df.pivot_table(\n",
    "        index=['article_id', 'article_title'],\n",
    "        columns='model_description',\n",
    "        values='commandline_count',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMBINED COMPARISON - ALL MODELS\")\n",
    "    print(\"=\"*80)\n",
    "    display(combined_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to JSON\n",
    "output_dir = PROJECT_ROOT / \"outputs\" / \"notebook_results\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = output_dir / f\"commandline_observables_{timestamp}.json\"\n",
    "\n",
    "export_data = {\n",
    "    'timestamp': timestamp,\n",
    "    'test_config': {\n",
    "        'temperature': TEMPERATURE,\n",
    "        'seed': SEED,\n",
    "        'junk_filter_threshold': JUNK_FILTER_THRESHOLD,\n",
    "        'llm_models_tested': LLM_MODELS_TO_TEST,\n",
    "        'embedding_models_tested': EMBEDDING_MODELS_TO_TEST,\n",
    "        'article_ids': [a['id'] for a in articles]\n",
    "    },\n",
    "    'results': results\n",
    "}\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "\n",
    "print(f\"✅ Results exported to: {output_file}\")\n",
    "\n",
    "# Also export as CSV\n",
    "csv_file = output_dir / f\"commandline_observables_{timestamp}.csv\"\n",
    "df.to_csv(csv_file, index=False)\n",
    "print(f\"✅ CSV exported to: {csv_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
