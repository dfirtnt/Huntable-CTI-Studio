{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTI-to-Hunt Logic Fine-tuning Environment Setup\n",
    "\n",
    "This notebook sets up the environment for fine-tuning a model to convert cyber threat intelligence text into concise hunt logic.\n",
    "\n",
    "## Hardware Requirements\n",
    "- **GPU**: RTX 4090 (24GB) or A100 (40GB) recommended\n",
    "- **RAM**: 32GB+ system memory\n",
    "- **Storage**: 100GB+ available space\n",
    "\n",
    "## Model Target\n",
    "- **Base Model**: Llama 3.1 8B Instruct\n",
    "- **Training Method**: QLoRA (4-bit quantized LoRA)\n",
    "- **Output Format**: 1-10 line hunt logic statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check if we're in the CTIScraper directory\n",
    "if not Path('./src').exists():\n",
    "    print(\"‚ö†Ô∏è  Warning: Not in CTIScraper root directory\")\n",
    "    print(\"Please navigate to the CTIScraper root directory\")\nelse:\n",
    "    print(\"‚úÖ In CTIScraper root directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPU and CUDA Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            gpu_name = torch.cuda.get_device_name(i)\n",
    "            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "            print(f\"  GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No CUDA GPUs detected. Training will be slow on CPU.\")\n",
    "        \nexcept ImportError:\n",
    "    print(\"‚ùå PyTorch not installed. Please install requirements first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install ML Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ML dependencies\n",
    "!pip install -r requirements-ml.txt\n",
    "\n",
    "# Download spaCy model for text processing\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verify Core Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test core library imports\n",
    "libraries = {\n",
    "    'torch': 'PyTorch',\n",
    "    'transformers': 'Hugging Face Transformers',\n",
    "    'datasets': 'Hugging Face Datasets',\n",
    "    'peft': 'PEFT (Parameter Efficient Fine-tuning)',\n",
    "    'bitsandbytes': 'BitsAndBytes (Quantization)',\n",
    "    'accelerate': 'Hugging Face Accelerate',\n",
    "    'trl': 'TRL (Transformer Reinforcement Learning)',\n",
    "    'wandb': 'Weights & Biases',\n",
    "    'evaluate': 'Hugging Face Evaluate'\n",
    "}\n",
    "\n",
    "print(\"Library verification:\")\n",
    "for lib, name in libraries.items():\n",
    "    try:\n",
    "        module = __import__(lib)\n",
    "        version = getattr(module, '__version__', 'unknown')\n",
    "        print(f\"‚úÖ {name}: {version}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå {name}: Failed to import - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Training Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory structure for training\n",
    "directories = [\n",
    "    'models',\n",
    "    'models/base',\n",
    "    'models/checkpoints',\n",
    "    'models/fine_tuned',\n",
    "    'data',\n",
    "    'data/raw',\n",
    "    'data/processed',\n",
    "    'data/training',\n",
    "    'notebooks/configs',\n",
    "    'logs',\n",
    "    'outputs'\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"‚úÖ Created directory: {directory}\")\n",
    "\n",
    "print(\"\\nTraining directory structure created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Model Loading (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Test loading a small model to verify everything works\n",
    "test_model = input(\"Test model loading? (y/n): \").lower().strip()\n",
    "\n",
    "if test_model == 'y':\n",
    "    print(\"Testing model loading with a small model...\")\n",
    "    \n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    import torch\n",
    "    \n",
    "    # Test with a small model first\n",
    "    model_name = \"microsoft/DialoGPT-small\"\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Successfully loaded test model: {model_name}\")\n",
    "        print(f\"   Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "        print(f\"   Model device: {next(model.parameters()).device}\")\n",
    "        \n",
    "        # Clean up\n",
    "        del model, tokenizer\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load test model: {e}\")\nelse:\n",
    "    print(\"Skipping model loading test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize Weights & Biases (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb for experiment tracking\n",
    "setup_wandb = input(\"Setup Weights & Biases for experiment tracking? (y/n): \").lower().strip()\n",
    "\n",
    "if setup_wandb == 'y':\n",
    "    import wandb\n",
    "    \n",
    "    print(\"Please log in to Weights & Biases:\")\n",
    "    wandb.login()\n",
    "    \n",
    "    # Test connection\n",
    "    test_run = wandb.init(\n",
    "        project=\"cti-hunt-logic-fine-tuning\",\n",
    "        name=\"environment-setup-test\",\n",
    "        job_type=\"setup\"\n",
    "    )\n",
    "    \n",
    "    wandb.log({\"setup_status\": \"complete\"})\n",
    "    wandb.finish()\n",
    "    \n",
    "    print(\"‚úÖ Weights & Biases setup complete!\")\nelse:\n",
    "    print(\"Skipping Weights & Biases setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Environment Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ Environment Setup Complete!\")\nprint(\"\\n\" + \"=\"*50)\nprint(\"SUMMARY\")\nprint(\"=\"*50)\nprint(f\"‚úÖ Python {sys.version.split()[0]}\")\nprint(f\"‚úÖ PyTorch {torch.__version__} with CUDA {torch.cuda.is_available()}\")\nprint(f\"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\" if torch.cuda.is_available() else \"‚ùå No GPU detected\")\nprint(\"‚úÖ All required libraries installed\")\nprint(\"‚úÖ Directory structure created\")\nprint(\"\\nüìù Next Steps:\")\nprint(\"1. Run notebook 02_data_preparation.ipynb\")\nprint(\"2. Prepare your CTI training data\")\nprint(\"3. Begin model fine-tuning\")\nprint(\"\\n\" + \"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}