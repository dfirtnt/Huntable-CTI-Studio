{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CTIScraper Fine-tuning on Colab T4 GPU\n",
        "\n",
        "This notebook runs fine-tuning on Google Colab's free T4 GPU while integrating with your local UI.\n",
        "\n",
        "## Setup Instructions:\n",
        "1. Enable GPU: Runtime \u2192 Change runtime type \u2192 GPU \u2192 T4\n",
        "2. Run all cells to set up the environment\n",
        "3. Use your local UI to trigger training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install transformers torch datasets accelerate huggingface_hub\n",
        "!pip install -q colabcode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import sys\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "import json\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Check GPU availability\n",
        "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup Hugging Face Hub authentication\n",
        "def setup_huggingface_hub():\n",
        "    \"\"\"Setup Hugging Face Hub authentication\"\"\"\n",
        "    try:\n",
        "        HF_TOKEN = \"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\"  # Your token\n",
        "        login(token=HF_TOKEN)\n",
        "        print(\"\u2705 Hugging Face Hub authentication successful\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Hugging Face Hub setup failed: {e}\")\n",
        "        return False\n",
        "\n",
        "setup_huggingface_hub()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fine-tuning trainer class\n",
        "class FineTuningTrainer:\n",
        "    def __init__(self, device=\"auto\"):\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        \n",
        "    def load_model(self, model_name):\n",
        "        \"\"\"Load model and tokenizer\"\"\"\n",
        "        print(f\"Loading model: {model_name}\")\n",
        "        \n",
        "        # Load tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        \n",
        "        # Load model\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
        "        )\n",
        "        \n",
        "        print(f\"\u2705 Model loaded successfully\")\n",
        "        return self.model, self.tokenizer\n",
        "    \n",
        "    def create_training_data(self, training_data):\n",
        "        \"\"\"Create training dataset from input data\"\"\"\n",
        "        def tokenize_function(examples):\n",
        "            # Format: \"<|user|>\\n{input}<|end|>\\n<|assistant|>\\n{output}<|end|>\"\n",
        "            texts = []\n",
        "            for input_text, output_text in zip(examples['input'], examples['output']):\n",
        "                formatted_text = f\"<|user|>\\n{input_text}<|end|>\\n<|assistant|>\\n{output_text}<|end|>\"\n",
        "                texts.append(formatted_text)\n",
        "            \n",
        "            tokenized = self.tokenizer(\n",
        "                texts,\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=512,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            \n",
        "            # For causal LM, labels are the same as input_ids\n",
        "            tokenized['labels'] = tokenized['input_ids'].clone()\n",
        "            return tokenized\n",
        "        \n",
        "        # Convert to Hugging Face dataset\n",
        "        dataset = Dataset.from_dict({\n",
        "            'input': [item['input'] for item in training_data],\n",
        "            'output': [item['output'] for item in training_data]\n",
        "        })\n",
        "        \n",
        "        tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "        return tokenized_dataset\n",
        "    \n",
        "    def fine_tune_model(self, model_name, training_data, epochs=3, learning_rate=5e-5, \n",
        "                       output_dir=\"./models\", push_to_hub=False, hub_model_id=None):\n",
        "        \"\"\"Fine-tune the model\"\"\"\n",
        "        print(f\"\ud83d\ude80 Starting fine-tuning on Colab T4 GPU\")\n",
        "        print(f\"Model: {model_name}\")\n",
        "        print(f\"Training examples: {len(training_data)}\")\n",
        "        print(f\"Epochs: {epochs}\")\n",
        "        print(f\"Learning rate: {learning_rate}\")\n",
        "        \n",
        "        # Load model\n",
        "        model, tokenizer = self.load_model(model_name)\n",
        "        \n",
        "        # Create training data\n",
        "        train_dataset = self.create_training_data(training_data)\n",
        "        \n",
        "        # Training arguments optimized for T4\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            num_train_epochs=epochs,\n",
        "            per_device_train_batch_size=2,  # Small batch for T4\n",
        "            gradient_accumulation_steps=4,  # Effective batch size = 8\n",
        "            learning_rate=learning_rate,\n",
        "            warmup_steps=100,\n",
        "            logging_steps=10,\n",
        "            save_steps=500,\n",
        "            evaluation_strategy=\"no\",\n",
        "            save_strategy=\"steps\",\n",
        "            load_best_model_at_end=False,\n",
        "            report_to=None,  # Disable wandb\n",
        "            fp16=torch.cuda.is_available(),  # Use mixed precision on GPU\n",
        "            dataloader_pin_memory=False,\n",
        "            remove_unused_columns=False\n",
        "        )\n",
        "        \n",
        "        # Create trainer\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            tokenizer=tokenizer\n",
        "        )\n",
        "        \n",
        "        # Start training\n",
        "        print(\"\ud83d\udd25 Training started on T4 GPU...\")\n",
        "        trainer.train()\n",
        "        \n",
        "        # Save model\n",
        "        trainer.save_model()\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "        print(f\"\u2705 Model saved to {output_dir}\")\n",
        "        \n",
        "        # Push to Hugging Face Hub if requested\n",
        "        if push_to_hub and hub_model_id:\n",
        "            print(f\"\ud83d\udce4 Pushing model to Hugging Face Hub: {hub_model_id}\")\n",
        "            trainer.push_to_hub(hub_model_id)\n",
        "            print(f\"\u2705 Model pushed to: https://huggingface.co/{hub_model_id}\")\n",
        "        \n",
        "        return trainer, output_dir\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = FineTuningTrainer(device=\"auto\")\n",
        "print(\"\u2705 FineTuningTrainer initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Manual training trigger\n",
        "# Replace this with your actual training data\n",
        "training_data = [\n",
        "    {\n",
        "        \"input\": \"What is a phishing attack?\",\n",
        "        \"output\": \"A phishing attack is a type of social engineering attack where attackers impersonate legitimate entities to trick victims into revealing sensitive information.\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"How do I detect malware?\",\n",
        "        \"output\": \"You can detect malware through behavioral analysis, signature-based detection, heuristic analysis, and monitoring for suspicious network traffic patterns.\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"What is threat hunting?\",\n",
        "        \"output\": \"Threat hunting is the proactive search for threats and malicious activity within an organization's network that may have evaded existing security controls.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Start training\n",
        "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "hub_model_id = \"dfirtnt/test-cti-model-colab\"\n",
        "\n",
        "trainer_instance, output_path = trainer.fine_tune_model(\n",
        "    model_name=model_name,\n",
        "    training_data=training_data,\n",
        "    epochs=1,  # Quick test\n",
        "    learning_rate=5e-5,\n",
        "    output_dir=\"./models/colab_test\",\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=hub_model_id\n",
        ")\n",
        "\n",
        "print(\"\ud83c\udf89 Training completed successfully!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}